{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "### and backward elimination\n",
    "\n",
    "The purpose of this notebook is to \n",
    "* apply Multiple Linear Regression to the preprocessed dataset\n",
    "* apply backward elimination to the model\n",
    "* ultimately find out the independent variables (World Development Idicators) which influence the dependent variable (Happy Planet Index) the most.\n",
    "\n",
    "The model will be applied to the \"wdi_hpi_2016_df\" dataset, which was created in the Data Preprocessing JNotebook. This dataset is based on\n",
    "* the Happy Planet Index for 2016 (see https://happyplanetindex.org/),\n",
    "* the World Development Indicators (1960 - 2019) by the World Bank (see https://datacatalog.worldbank.org/dataset/world-development-indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "dataset = pd.read_pickle('../data/wdi_hpi_2016_df.pkl')\n",
    "X = dataset.iloc[:, 1:-1].values\n",
    "y = dataset.iloc[:, dataset.shape[1]-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datasets into Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Multiple Linear Regression Model to the Training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Test set results\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 12 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>4.13e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:09:34</td>     <th>  Log-Likelihood:    </th> <td> -307.15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   139</td>      <th>  AIC:               </th> <td>   808.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    42</td>      <th>  BIC:               </th> <td>   1093.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    96</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -26.2703</td> <td>   44.902</td> <td>   -0.585</td> <td> 0.562</td> <td> -116.886</td> <td>   64.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0054</td> <td>    0.003</td> <td>    2.120</td> <td> 0.040</td> <td>    0.000</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0196</td> <td>    0.040</td> <td>   -0.489</td> <td> 0.627</td> <td>   -0.101</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.1998</td> <td>    0.166</td> <td>   -1.206</td> <td> 0.235</td> <td>   -0.534</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0351</td> <td>    0.039</td> <td>   -0.899</td> <td> 0.374</td> <td>   -0.114</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0623</td> <td>    0.040</td> <td>   -1.546</td> <td> 0.130</td> <td>   -0.144</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0338</td> <td>    0.031</td> <td>    1.101</td> <td> 0.277</td> <td>   -0.028</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.0560</td> <td>    0.053</td> <td>   -1.050</td> <td> 0.300</td> <td>   -0.164</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.0196</td> <td>    0.069</td> <td>   -0.282</td> <td> 0.779</td> <td>   -0.160</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.2253</td> <td>    0.153</td> <td>    1.469</td> <td> 0.149</td> <td>   -0.084</td> <td>    0.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.0057</td> <td>    0.076</td> <td>   -0.075</td> <td> 0.940</td> <td>   -0.160</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0463</td> <td>    0.100</td> <td>   -0.463</td> <td> 0.646</td> <td>   -0.248</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> 7.738e-12</td> <td> 8.65e-12</td> <td>    0.894</td> <td> 0.376</td> <td>-9.73e-12</td> <td> 2.52e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> 5.451e-11</td> <td> 3.32e-11</td> <td>    1.641</td> <td> 0.108</td> <td>-1.25e-11</td> <td> 1.22e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>-8.277e-12</td> <td>  1.7e-10</td> <td>   -0.049</td> <td> 0.961</td> <td>-3.52e-10</td> <td> 3.35e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.1027</td> <td>    0.126</td> <td>    0.812</td> <td> 0.421</td> <td>   -0.152</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0389</td> <td>    0.050</td> <td>    0.783</td> <td> 0.438</td> <td>   -0.061</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> 9.053e-10</td> <td> 4.57e-10</td> <td>    1.982</td> <td> 0.054</td> <td>-1.63e-11</td> <td> 1.83e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.0076</td> <td>    0.009</td> <td>   -0.805</td> <td> 0.425</td> <td>   -0.027</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>   -0.1230</td> <td>    0.073</td> <td>   -1.686</td> <td> 0.099</td> <td>   -0.270</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0014</td> <td>    0.002</td> <td>    0.591</td> <td> 0.558</td> <td>   -0.003</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.1179</td> <td>    0.101</td> <td>    1.166</td> <td> 0.250</td> <td>   -0.086</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>   -0.0499</td> <td>    0.066</td> <td>   -0.755</td> <td> 0.455</td> <td>   -0.184</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.1460</td> <td>    0.129</td> <td>   -1.129</td> <td> 0.265</td> <td>   -0.407</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0004</td> <td>    0.023</td> <td>    0.019</td> <td> 0.985</td> <td>   -0.045</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.1036</td> <td>    0.331</td> <td>    0.312</td> <td> 0.756</td> <td>   -0.565</td> <td>    0.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0040</td> <td>    0.067</td> <td>   -0.059</td> <td> 0.953</td> <td>   -0.138</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.0122</td> <td>    0.023</td> <td>   -0.528</td> <td> 0.600</td> <td>   -0.059</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>    0.0033</td> <td>    0.027</td> <td>    0.121</td> <td> 0.904</td> <td>   -0.052</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.1713</td> <td>    0.255</td> <td>   -0.673</td> <td> 0.505</td> <td>   -0.685</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -1.1039</td> <td>    1.333</td> <td>   -0.828</td> <td> 0.412</td> <td>   -3.793</td> <td>    1.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.2351</td> <td>    0.109</td> <td>    2.160</td> <td> 0.037</td> <td>    0.015</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    0.1986</td> <td>    0.097</td> <td>    2.043</td> <td> 0.047</td> <td>    0.002</td> <td>    0.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.5576</td> <td>    0.170</td> <td>    3.283</td> <td> 0.002</td> <td>    0.215</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -1.1936</td> <td>    0.438</td> <td>   -2.728</td> <td> 0.009</td> <td>   -2.077</td> <td>   -0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.7916</td> <td>    0.300</td> <td>    2.643</td> <td> 0.012</td> <td>    0.187</td> <td>    1.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>    0.0145</td> <td>    0.109</td> <td>    0.133</td> <td> 0.894</td> <td>   -0.205</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -0.0381</td> <td>    0.067</td> <td>   -0.564</td> <td> 0.576</td> <td>   -0.174</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>   -0.6832</td> <td>    0.196</td> <td>   -3.490</td> <td> 0.001</td> <td>   -1.078</td> <td>   -0.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>    0.0048</td> <td>    0.012</td> <td>    0.395</td> <td> 0.695</td> <td>   -0.020</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>   -0.0270</td> <td>    0.079</td> <td>   -0.340</td> <td> 0.735</td> <td>   -0.187</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.0737</td> <td>    0.093</td> <td>    0.789</td> <td> 0.435</td> <td>   -0.115</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    0.0336</td> <td>    0.035</td> <td>    0.961</td> <td> 0.342</td> <td>   -0.037</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td> -4.21e-05</td> <td>  8.1e-05</td> <td>   -0.520</td> <td> 0.606</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>-1.189e-05</td> <td> 8.52e-05</td> <td>   -0.140</td> <td> 0.890</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>-4.436e-07</td> <td> 1.78e-05</td> <td>   -0.025</td> <td> 0.980</td> <td>-3.64e-05</td> <td> 3.55e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>    0.0092</td> <td>    0.028</td> <td>    0.330</td> <td> 0.743</td> <td>   -0.047</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.2708</td> <td>    0.098</td> <td>   -2.777</td> <td> 0.008</td> <td>   -0.468</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    0.3652</td> <td>    0.143</td> <td>    2.546</td> <td> 0.015</td> <td>    0.076</td> <td>    0.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>   -0.8274</td> <td>    0.460</td> <td>   -1.798</td> <td> 0.079</td> <td>   -1.756</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   -0.0071</td> <td>    0.154</td> <td>   -0.046</td> <td> 0.963</td> <td>   -0.317</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>   -0.2178</td> <td>    0.246</td> <td>   -0.885</td> <td> 0.381</td> <td>   -0.714</td> <td>    0.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>   -0.0243</td> <td>    0.021</td> <td>   -1.183</td> <td> 0.244</td> <td>   -0.066</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.0634</td> <td>    0.056</td> <td>    1.126</td> <td> 0.266</td> <td>   -0.050</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>    0.0148</td> <td>    0.025</td> <td>    0.586</td> <td> 0.561</td> <td>   -0.036</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>   -0.3914</td> <td>    0.241</td> <td>   -1.626</td> <td> 0.111</td> <td>   -0.877</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>   -0.0040</td> <td>    0.174</td> <td>   -0.023</td> <td> 0.982</td> <td>   -0.356</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>   -0.2544</td> <td>    0.217</td> <td>   -1.171</td> <td> 0.248</td> <td>   -0.693</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>   -0.0445</td> <td>    0.110</td> <td>   -0.403</td> <td> 0.689</td> <td>   -0.267</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>   -0.0568</td> <td>    0.115</td> <td>   -0.495</td> <td> 0.623</td> <td>   -0.288</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>   -0.1073</td> <td>    0.071</td> <td>   -1.519</td> <td> 0.136</td> <td>   -0.250</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>   -0.1525</td> <td>    0.151</td> <td>   -1.013</td> <td> 0.317</td> <td>   -0.456</td> <td>    0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>   -0.4296</td> <td>    0.206</td> <td>   -2.090</td> <td> 0.043</td> <td>   -0.844</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>   -0.4192</td> <td>    0.173</td> <td>   -2.419</td> <td> 0.020</td> <td>   -0.769</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>   -0.0584</td> <td>    0.198</td> <td>   -0.295</td> <td> 0.769</td> <td>   -0.458</td> <td>    0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>    0.1608</td> <td>    0.134</td> <td>    1.203</td> <td> 0.236</td> <td>   -0.109</td> <td>    0.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>    0.5364</td> <td>    0.250</td> <td>    2.146</td> <td> 0.038</td> <td>    0.032</td> <td>    1.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>   -0.0002</td> <td>    0.000</td> <td>   -2.040</td> <td> 0.048</td> <td>   -0.000</td> <td>-2.62e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>    0.0345</td> <td>    0.295</td> <td>    0.117</td> <td> 0.908</td> <td>   -0.561</td> <td>    0.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   -3.0891</td> <td>   27.390</td> <td>   -0.113</td> <td> 0.911</td> <td>  -58.363</td> <td>   52.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>  -37.3922</td> <td>   35.735</td> <td>   -1.046</td> <td> 0.301</td> <td> -109.508</td> <td>   34.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>    5.7967</td> <td>   15.450</td> <td>    0.375</td> <td> 0.709</td> <td>  -25.383</td> <td>   36.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>    3.3303</td> <td>    5.266</td> <td>    0.632</td> <td> 0.531</td> <td>   -7.296</td> <td>   13.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>   -1.0473</td> <td>    1.102</td> <td>   -0.951</td> <td> 0.347</td> <td>   -3.271</td> <td>    1.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>   -0.0775</td> <td>    0.065</td> <td>   -1.201</td> <td> 0.236</td> <td>   -0.208</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>   44.9871</td> <td>   50.188</td> <td>    0.896</td> <td> 0.375</td> <td>  -56.297</td> <td>  146.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>  -21.7566</td> <td>   24.369</td> <td>   -0.893</td> <td> 0.377</td> <td>  -70.936</td> <td>   27.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>  -23.1829</td> <td>   25.819</td> <td>   -0.898</td> <td> 0.374</td> <td>  -75.287</td> <td>   28.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>    2.4523</td> <td>    1.436</td> <td>    1.708</td> <td> 0.095</td> <td>   -0.445</td> <td>    5.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>   13.8023</td> <td>   11.403</td> <td>    1.210</td> <td> 0.233</td> <td>   -9.210</td> <td>   36.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>   13.7000</td> <td>   11.800</td> <td>    1.161</td> <td> 0.252</td> <td>  -10.113</td> <td>   37.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>  -27.3067</td> <td>   23.210</td> <td>   -1.177</td> <td> 0.246</td> <td>  -74.146</td> <td>   19.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>    0.1367</td> <td>    0.126</td> <td>    1.081</td> <td> 0.286</td> <td>   -0.119</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>    0.0028</td> <td>    0.523</td> <td>    0.005</td> <td> 0.996</td> <td>   -1.053</td> <td>    1.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>   -0.0292</td> <td>    0.313</td> <td>   -0.093</td> <td> 0.926</td> <td>   -0.662</td> <td>    0.603</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td>   -0.0304</td> <td>    0.275</td> <td>   -0.111</td> <td> 0.912</td> <td>   -0.585</td> <td>    0.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>    0.2405</td> <td>    0.314</td> <td>    0.767</td> <td> 0.447</td> <td>   -0.392</td> <td>    0.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td>    0.1977</td> <td>    0.982</td> <td>    0.201</td> <td> 0.841</td> <td>   -1.784</td> <td>    2.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>   -0.0405</td> <td>    0.155</td> <td>   -0.262</td> <td> 0.795</td> <td>   -0.353</td> <td>    0.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>    0.0750</td> <td>    0.073</td> <td>    1.024</td> <td> 0.312</td> <td>   -0.073</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>   -0.0676</td> <td>    0.120</td> <td>   -0.562</td> <td> 0.577</td> <td>   -0.310</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>   -0.0521</td> <td>    0.039</td> <td>   -1.336</td> <td> 0.189</td> <td>   -0.131</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>    0.4627</td> <td>    0.879</td> <td>    0.526</td> <td> 0.602</td> <td>   -1.312</td> <td>    2.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>    0.0284</td> <td>    0.069</td> <td>    0.412</td> <td> 0.683</td> <td>   -0.111</td> <td>    0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>    1.1856</td> <td>    0.346</td> <td>    3.425</td> <td> 0.001</td> <td>    0.487</td> <td>    1.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>    0.4122</td> <td>    0.860</td> <td>    0.479</td> <td> 0.634</td> <td>   -1.324</td> <td>    2.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>-2.694e-08</td> <td> 7.23e-08</td> <td>   -0.373</td> <td> 0.711</td> <td>-1.73e-07</td> <td> 1.19e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>-1.192e-07</td> <td>    1e-07</td> <td>   -1.191</td> <td> 0.240</td> <td>-3.21e-07</td> <td> 8.28e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>    0.1391</td> <td>    0.229</td> <td>    0.607</td> <td> 0.547</td> <td>   -0.323</td> <td>    0.601</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.101</td> <th>  Durbin-Watson:     </th> <td>   1.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.951</td> <th>  Jarque-Bera (JB):  </th> <td>   0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.051</td> <th>  Prob(JB):          </th> <td>   0.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.832</td> <th>  Cond. No.          </th> <td>1.09e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.09e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.908\n",
       "Model:                            OLS   Adj. R-squared:                  0.699\n",
       "Method:                 Least Squares   F-statistic:                     4.339\n",
       "Date:                Wed, 12 Feb 2020   Prob (F-statistic):           4.13e-07\n",
       "Time:                        18:09:34   Log-Likelihood:                -307.15\n",
       "No. Observations:                 139   AIC:                             808.3\n",
       "Df Residuals:                      42   BIC:                             1093.\n",
       "Df Model:                          96                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -26.2703     44.902     -0.585      0.562    -116.886      64.345\n",
       "x1             0.0054      0.003      2.120      0.040       0.000       0.011\n",
       "x2            -0.0196      0.040     -0.489      0.627      -0.101       0.061\n",
       "x3            -0.1998      0.166     -1.206      0.235      -0.534       0.135\n",
       "x4            -0.0351      0.039     -0.899      0.374      -0.114       0.044\n",
       "x5            -0.0623      0.040     -1.546      0.130      -0.144       0.019\n",
       "x6             0.0338      0.031      1.101      0.277      -0.028       0.096\n",
       "x7            -0.0560      0.053     -1.050      0.300      -0.164       0.052\n",
       "x8            -0.0196      0.069     -0.282      0.779      -0.160       0.120\n",
       "x9             0.2253      0.153      1.469      0.149      -0.084       0.535\n",
       "x10           -0.0057      0.076     -0.075      0.940      -0.160       0.148\n",
       "x11           -0.0463      0.100     -0.463      0.646      -0.248       0.156\n",
       "x12         7.738e-12   8.65e-12      0.894      0.376   -9.73e-12    2.52e-11\n",
       "x13         5.451e-11   3.32e-11      1.641      0.108   -1.25e-11    1.22e-10\n",
       "x14        -8.277e-12    1.7e-10     -0.049      0.961   -3.52e-10    3.35e-10\n",
       "x15            0.1027      0.126      0.812      0.421      -0.152       0.358\n",
       "x16            0.0389      0.050      0.783      0.438      -0.061       0.139\n",
       "x17         9.053e-10   4.57e-10      1.982      0.054   -1.63e-11    1.83e-09\n",
       "x18           -0.0076      0.009     -0.805      0.425      -0.027       0.011\n",
       "x19           -0.1230      0.073     -1.686      0.099      -0.270       0.024\n",
       "x20            0.0014      0.002      0.591      0.558      -0.003       0.006\n",
       "x21            0.1179      0.101      1.166      0.250      -0.086       0.322\n",
       "x22           -0.0499      0.066     -0.755      0.455      -0.184       0.084\n",
       "x23           -0.1460      0.129     -1.129      0.265      -0.407       0.115\n",
       "x24            0.0004      0.023      0.019      0.985      -0.045       0.046\n",
       "x25            0.1036      0.331      0.312      0.756      -0.565       0.772\n",
       "x26           -0.0040      0.067     -0.059      0.953      -0.138       0.130\n",
       "x27           -0.0122      0.023     -0.528      0.600      -0.059       0.035\n",
       "x28            0.0033      0.027      0.121      0.904      -0.052       0.058\n",
       "x29           -0.1713      0.255     -0.673      0.505      -0.685       0.342\n",
       "x30           -1.1039      1.333     -0.828      0.412      -3.793       1.585\n",
       "x31            0.2351      0.109      2.160      0.037       0.015       0.455\n",
       "x32            0.1986      0.097      2.043      0.047       0.002       0.395\n",
       "x33            0.5576      0.170      3.283      0.002       0.215       0.900\n",
       "x34           -1.1936      0.438     -2.728      0.009      -2.077      -0.311\n",
       "x35            0.7916      0.300      2.643      0.012       0.187       1.396\n",
       "x36            0.0145      0.109      0.133      0.894      -0.205       0.234\n",
       "x37           -0.0381      0.067     -0.564      0.576      -0.174       0.098\n",
       "x38           -0.6832      0.196     -3.490      0.001      -1.078      -0.288\n",
       "x39            0.0048      0.012      0.395      0.695      -0.020       0.029\n",
       "x40           -0.0270      0.079     -0.340      0.735      -0.187       0.133\n",
       "x41            0.0737      0.093      0.789      0.435      -0.115       0.262\n",
       "x42            0.0336      0.035      0.961      0.342      -0.037       0.104\n",
       "x43         -4.21e-05    8.1e-05     -0.520      0.606      -0.000       0.000\n",
       "x44        -1.189e-05   8.52e-05     -0.140      0.890      -0.000       0.000\n",
       "x45        -4.436e-07   1.78e-05     -0.025      0.980   -3.64e-05    3.55e-05\n",
       "x46            0.0092      0.028      0.330      0.743      -0.047       0.065\n",
       "x47           -0.2708      0.098     -2.777      0.008      -0.468      -0.074\n",
       "x48            0.3652      0.143      2.546      0.015       0.076       0.655\n",
       "x49           -0.8274      0.460     -1.798      0.079      -1.756       0.101\n",
       "x50           -0.0071      0.154     -0.046      0.963      -0.317       0.303\n",
       "x51           -0.2178      0.246     -0.885      0.381      -0.714       0.279\n",
       "x52           -0.0243      0.021     -1.183      0.244      -0.066       0.017\n",
       "x53            0.0634      0.056      1.126      0.266      -0.050       0.177\n",
       "x54            0.0148      0.025      0.586      0.561      -0.036       0.066\n",
       "x55           -0.3914      0.241     -1.626      0.111      -0.877       0.094\n",
       "x56           -0.0040      0.174     -0.023      0.982      -0.356       0.348\n",
       "x57           -0.2544      0.217     -1.171      0.248      -0.693       0.184\n",
       "x58           -0.0445      0.110     -0.403      0.689      -0.267       0.178\n",
       "x59           -0.0568      0.115     -0.495      0.623      -0.288       0.175\n",
       "x60           -0.1073      0.071     -1.519      0.136      -0.250       0.035\n",
       "x61           -0.1525      0.151     -1.013      0.317      -0.456       0.151\n",
       "x62           -0.4296      0.206     -2.090      0.043      -0.844      -0.015\n",
       "x63           -0.4192      0.173     -2.419      0.020      -0.769      -0.070\n",
       "x64           -0.0584      0.198     -0.295      0.769      -0.458       0.341\n",
       "x65            0.1608      0.134      1.203      0.236      -0.109       0.431\n",
       "x66            0.5364      0.250      2.146      0.038       0.032       1.041\n",
       "x67           -0.0002      0.000     -2.040      0.048      -0.000   -2.62e-06\n",
       "x68            0.0345      0.295      0.117      0.908      -0.561       0.630\n",
       "x69           -3.0891     27.390     -0.113      0.911     -58.363      52.185\n",
       "x70          -37.3922     35.735     -1.046      0.301    -109.508      34.723\n",
       "x71            5.7967     15.450      0.375      0.709     -25.383      36.976\n",
       "x72            3.3303      5.266      0.632      0.531      -7.296      13.957\n",
       "x73           -1.0473      1.102     -0.951      0.347      -3.271       1.176\n",
       "x74           -0.0775      0.065     -1.201      0.236      -0.208       0.053\n",
       "x75           44.9871     50.188      0.896      0.375     -56.297     146.271\n",
       "x76          -21.7566     24.369     -0.893      0.377     -70.936      27.422\n",
       "x77          -23.1829     25.819     -0.898      0.374     -75.287      28.921\n",
       "x78            2.4523      1.436      1.708      0.095      -0.445       5.349\n",
       "x79           13.8023     11.403      1.210      0.233      -9.210      36.815\n",
       "x80           13.7000     11.800      1.161      0.252     -10.113      37.513\n",
       "x81          -27.3067     23.210     -1.177      0.246     -74.146      19.533\n",
       "x82            0.1367      0.126      1.081      0.286      -0.119       0.392\n",
       "x83            0.0028      0.523      0.005      0.996      -1.053       1.059\n",
       "x84           -0.0292      0.313     -0.093      0.926      -0.662       0.603\n",
       "x85           -0.0304      0.275     -0.111      0.912      -0.585       0.525\n",
       "x86            0.2405      0.314      0.767      0.447      -0.392       0.873\n",
       "x87            0.1977      0.982      0.201      0.841      -1.784       2.179\n",
       "x88           -0.0405      0.155     -0.262      0.795      -0.353       0.272\n",
       "x89            0.0750      0.073      1.024      0.312      -0.073       0.223\n",
       "x90           -0.0676      0.120     -0.562      0.577      -0.310       0.175\n",
       "x91           -0.0521      0.039     -1.336      0.189      -0.131       0.027\n",
       "x92            0.4627      0.879      0.526      0.602      -1.312       2.237\n",
       "x93            0.0284      0.069      0.412      0.683      -0.111       0.167\n",
       "x94            1.1856      0.346      3.425      0.001       0.487       1.884\n",
       "x95            0.4122      0.860      0.479      0.634      -1.324       2.148\n",
       "x96        -2.694e-08   7.23e-08     -0.373      0.711   -1.73e-07    1.19e-07\n",
       "x97        -1.192e-07      1e-07     -1.191      0.240   -3.21e-07    8.28e-08\n",
       "x98            0.1391      0.229      0.607      0.547      -0.323       0.601\n",
       "==============================================================================\n",
       "Omnibus:                        0.101   Durbin-Watson:                   1.927\n",
       "Prob(Omnibus):                  0.951   Jarque-Bera (JB):                0.226\n",
       "Skew:                           0.051   Prob(JB):                        0.893\n",
       "Kurtosis:                       2.832   Cond. No.                     1.09e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.09e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce less important variables with Backward Elimination\n",
    "import statsmodels.regression.linear_model as sm\n",
    "# for statsmodel to understand the multiple linear regression equation a new column with b0 equals one is required (y = b0 + b1*x1 + b2*x2 + ... + bn*xn)\n",
    "X_opt = np.append(arr = np.ones((len(X), 1)).astype(int), values = X, axis = 1)   # Add X to the newly created array of 1s\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef backward_elimination(x, significance):\\n    num_vars = len(x[0])\\n    temp = np.zeros((X_opt.shape)).astype(int)\\n    for i in range(0, num_vars):\\n        regressor_OLS = sm.OLS(endog = y, exog = x).fit()\\n        max_var = max(regressor_OLS.pvalues).astype(float)\\n        adjR_before = regressor_OLS.rsquared_adj.astype(float)\\n        if max_var > significance:\\n            for j in range(0, num_vars - i):\\n                if (regressor_OLS.pvalues[j].astype(float) == max_var):\\n                    temp[:,j] = x[:, j]\\n                    x = np.delete(x, j, axis = 1)\\n                    tmp_regressor = sm.OLS(endog = y, exog = x).fit()\\n                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\\n                    if (adjR_before >= adjR_after):\\n                        x_rollback = np.hstack((x, temp[:,[0,j]]))\\n                        x_rollback = np.delete(x_rollback, j, 1)\\n                        print (regressor_OLS.summary())\\n                        return x_rollback\\n                    else:\\n                        continue\\n    regressor_OLS.summary()\\n    return x\\n\\nsignificance = 0.05\\nX_opt = X_opt[:, list(range(X_opt.shape[1]))]\\nX_opt2 = backward_elimination(X_opt, significance)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fuction to automatically remove columns where P-value is below significance level of 5%\n",
    "'''\n",
    "def backward_elimination(x, significance):\n",
    "    num_vars = len(x[0])\n",
    "    temp = np.zeros((X_opt.shape)).astype(int)\n",
    "    for i in range(0, num_vars):\n",
    "        regressor_OLS = sm.OLS(endog = y, exog = x).fit()\n",
    "        max_var = max(regressor_OLS.pvalues).astype(float)\n",
    "        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        if max_var > significance:\n",
    "            for j in range(0, num_vars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == max_var):\n",
    "                    temp[:,j] = x[:, j]\n",
    "                    x = np.delete(x, j, axis = 1)\n",
    "                    tmp_regressor = sm.OLS(endog = y, exog = x).fit()\n",
    "                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                    if (adjR_before >= adjR_after):\n",
    "                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n",
    "                        x_rollback = np.delete(x_rollback, j, 1)\n",
    "                        print (regressor_OLS.summary())\n",
    "                        return x_rollback\n",
    "                    else:\n",
    "                        continue\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    "\n",
    "significance = 0.05\n",
    "X_opt = X_opt[:, list(range(X_opt.shape[1]))]\n",
    "X_opt2 = backward_elimination(X_opt, significance)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 39)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, sl):\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    "         \n",
    "SL = 0.05\n",
    "X_opt = X_opt[:, list(range(X_opt.shape[1]))]\n",
    "X_Modeled = backwardElimination(X_opt, SL)\n",
    "X_Modeled.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
